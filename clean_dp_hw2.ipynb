{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# CS590 Data Security and Privacy\n",
    "## Homework 4 Differential Privacy II"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load the data and libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "from unittest.mock import patch\n",
    "\n",
    "\n",
    "adult = pd.read_csv('https://github.com/jnear/cs211-data-privacy/raw/master/homework/adult_with_pii.csv')\n",
    "adult = adult.dropna()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# varify the downloaded adult dataset\n",
    "adult.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Question 1: Exponential mechanism (20 points)\n",
    "- (5 points) If one wants to get the `Education` group with the maximum number of records with `Age` > 50 with exponential mechanism,\n",
    "how to design the scores? What is the sensitivity of your score?\n",
    "\n",
    "    For consistency, we can denote the score of output $r$ based on dataset $X$ as $u(X, r)$"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "YOUR ANSWER HERE"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- (10 points) Implement a differential private function based on basic exponential mechanism to return the `Education` group with the maximum number of records with `Age` > 50.\n",
    "Clip the `Age` to be in [0, 100]. Also, explain how to set the sensitivity.\n",
    "\n",
    "    *Recall that the probability of one output is selected is proportional to $\\exp(\\frac{\\epsilon u(X, r)}{2\\Delta u}  )$"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def basic_exponential(epsilon):\n",
    "    # YOUR CODE HERE\n",
    "    # ANSWER\n",
    "\n",
    "    return None\n",
    "\n",
    "print(f\"The groun with smallest number of older than 50 is {basic_exponential(epsilon=1)}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- (5 points) Can we improve the basic exponential mechanism for the above query? Explain  1. how to improve if your answer is yes 1. why you can make the changes for this query.\n",
    "\n",
    "    Hint: Some reference for this questions can be\n",
    "    1. Section 3.4 of \"The Algorithmic Foundations of Differential Privacy\": https://www.cis.upenn.edu/~aaroth/Papers/privacybook.pdf\n",
    "    2. Section 2 of \"Understanding the Sparse Vector Technique for Differential Privacy\": http://www.vldb.org/pvldb/vol10/p637-lyu.pdf"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "YOUR ANSWER HERE"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Question 2: DP with machine learning - Gaussian mechanism and gradient perturbation (40 points)\n",
    "\n",
    "In this question use regularized logistic regression (LR) to predict the `Target` column, which indicate whether the individual has income greate than 50k.\n",
    "\n",
    "The dataset is pre-processed and loaded by the code below.\n",
    "For classification, we take the `Target` value as label $y$, where $y=-1$ if <=50k, otherwise it is +1.\n",
    "Denote the dataset with $n$ samples as $X = [x_1, \\ldots, x_n]$ and each $x_i \\in \\mathbb{R}^d$.\n",
    "With logistic regression, we represent the probability of label $y$ being either $-1$ or $1$ given parameter vector $w$ and features $x$ as\n",
    "$\\mathbb{P}(y|w, x) = \\frac{1}{1 + e^{-y w^T x}}$, where $w^T x$ is the inner product of $w$ and $x$.\n",
    "\n",
    "The training process is to maximize the (log)likelihood of obtaining such dataset with $w$.\n",
    "$L(X) = -\\log(\\prod_{i=1}^{n}\\mathbb{P}(y_i|w, x_i)) = \\frac{1}{n}\\sum_{i=1}^{n}\n",
    "\\log(1 + e^{-y w^T x} ) $.\n",
    "It is usually simplified by defining $\\ell(w; x, y) = \\log(1 + e^{-y w^T x} ) $\n",
    "\n",
    "(Notice that you may see another kind of loss function for LR, it is because their dataset lables are in $\\{0, 1\\}$ while we use $\\{-1, 1\\}$ here.)\n",
    "\n",
    "We are going to use gradient descent as the optimization algorithm to train $w$.\n",
    "It is not hard to see that the gradient of a sample is\n",
    "$\\frac{\\partial \\ell(w; x, y)}{\\partial w} = \\frac{-y x}{1 + e^{w^T x}}$.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load data files\n",
    "import numpy as np\n",
    "import urllib.request\n",
    "import io\n",
    "\n",
    "\n",
    "url_x = 'https://github.com/jnear/cs211-data-privacy/raw/master/slides/adult_processed_x.npy'\n",
    "url_y = 'https://github.com/jnear/cs211-data-privacy/raw/master/slides/adult_processed_y.npy'\n",
    "\n",
    "with urllib.request.urlopen(url_x) as url:\n",
    "    f = io.BytesIO(url.read())\n",
    "X = np.load(f)\n",
    "\n",
    "with urllib.request.urlopen(url_y) as url:\n",
    "    f = io.BytesIO(url.read())\n",
    "y = np.load(f)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Split data into training and test sets\n",
    "training_size = int(X.shape[0] * 0.8)\n",
    "\n",
    "X_train = X[:training_size]\n",
    "X_test = X[training_size:]\n",
    "\n",
    "y_train = y[:training_size]\n",
    "y_test = y[training_size:]\n",
    "\n",
    "print('Train and test set sizes:', len(y_train), len(y_test))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Some useful functions\n",
    "\n",
    "def loss(w, xi, yi):\n",
    "    exponent = - yi * (xi.dot(w))\n",
    "    return np.log(1 + np.exp(exponent))\n",
    "\n",
    "\n",
    "def gradient(w, xi, yi):\n",
    "    exponent = yi * (xi.dot(w))\n",
    "    return - (yi*xi) / (1+np.exp(exponent))\n",
    "\n",
    "\n",
    "def predict(xi, w, bias=0):\n",
    "    label = np.sign(xi @ w + bias)\n",
    "    return label\n",
    "\n",
    "\n",
    "def test_accuracy(w):\n",
    "    return np.sum(predict(X_test, w) == y_test)/X_test.shape[0]\n",
    "\n",
    "def train_accuracy(w):\n",
    "    return np.sum(predict(X_train, w) == y_train)/X_train.shape[0]\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- (5 points) First, let's implement a clean training of logistic regrssion with the training dataset (`X_train` and `y_train`).\n",
    "Fill in the missing part below and run the plotting code."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def gradient_descent(iterations, X_train, y_train):\n",
    "    w = np.zeros(X_train.shape[1])\n",
    "    # Strictly speaking, we need to apply Laplace mechanism to estimate the total number of users in dataset\n",
    "    n = X_train.shape[0]\n",
    "    # The following two are used to keep track of the training information\n",
    "    itr_losses = []\n",
    "    train_accuracies = []\n",
    "\n",
    "\n",
    "    for i in range(iterations):\n",
    "        cur_loss = np.average([loss(w, xi, yi) for xi, yi in zip(X_train,y_train)])\n",
    "        cur_acc = train_accuracy(w)\n",
    "        itr_losses.append(cur_loss)\n",
    "        train_accuracies.append(cur_acc)\n",
    "        if i % 10 == 0:\n",
    "            print(f\"training iteration: {i}\")\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "\n",
    "    return w, itr_losses, train_accuracies\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# use your function to train\n",
    "iterations = 50\n",
    "w, itr_losses, train_accuracies = gradient_descent(iterations, X_train, y_train)\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "ax1.plot(np.arange(iterations), itr_losses, label='training loss' )\n",
    "ax2.plot(np.arange(iterations), train_accuracies, label='training accuracy')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "* (5 points)  It is easy to see that the gradient w.r.t each sample is not bounded in the training iterations,\n",
    "so the sensitivity of DP for the gradient decent is not bounded as well.\n",
    "To encounter such problem, a typical solution is called \"gradient clipping\".\n",
    "We need to clip the gradient w.r.t EACH SAMPLE (NOT the average gradient!) so that its L2 norm is at most $\\theta$.\n",
    "Implement your `gradient_clipping` function below."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def gradient_clipping(raw_gradient, theta):\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    return None\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "* (5 points)  Implement the Gaussian mechanism with input parameters consist of a vector, sensitivity, $\\epsilon$, $\\delta$.\n",
    "The return should the vector with appropriated additive Gaussian noise."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def gaussian_mech_vec(v, sensitivity, epsilon, delta):\n",
    "    # YOUR CODE HERE\n",
    "    return None\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "* (10 points) Implement your private LR training function below.\n",
    "    - [Naive composition of privacy budgets] We consider only the naive sequntial composition of privacy budget here.\n",
    "    Namely, if there are two mechanisms and $M_1$ is ($\\epsilon_1, \\delta_1$)-private and $M_2$ is ($\\epsilon_2, \\delta_2$)-private,\n",
    "    then the privacy loss of $M_2(M_1(X, aux_1), X, aux_2)$ is ($\\epsilon_1 + \\epsilon_2, \\delta_1 + \\delta_2$)-private.\n",
    "    The $aux$ denotes any auxiliary input.\n",
    "    So if you decide there is $T$ iterations, each iteration should satisfy ($\\frac{\\epsilon}{T}, \\frac{\\delta}{T}$)-DP.\n",
    "    - [Decrease of step sizes] Different from the gradient descent above, the injected noise to the gradient can prevent $w$ to converge.\n",
    "    A solution is the step size decreases as the number of iteration increases, similar as Stochastic Gradient Descent (SGD).\n",
    "    You can let the step size decrease linearly in your implementation."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def dp_gradient_descent(iterations, X_train, y_train, epsilon, delta, theta):\n",
    "    w = np.zeros(X_train.shape[1])\n",
    "    # Strictly speaking, we need to apply Laplace mechanism to estimate the total number of users in dataset\n",
    "    n = X_train.shape[0]\n",
    "    # The following two are used to keep track of the training information\n",
    "    itr_losses = []\n",
    "    train_accuracies = []\n",
    "    step_size = 0.1\n",
    "\n",
    "    for i in range(iterations):\n",
    "        cur_loss = np.average([loss(w, xi, yi) for xi, yi in zip(X_train,y_train)])\n",
    "        cur_acc = train_accuracy(w)\n",
    "        itr_losses.append(cur_loss)\n",
    "        train_accuracies.append(cur_acc)\n",
    "        if i % 10 == 0:\n",
    "            print(f\"training iteration: {i}\")\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "\n",
    "\n",
    "    return w, itr_losses, train_accuracies"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# use your private function to train\n",
    "iterations = 10\n",
    "epsilon = 2\n",
    "delta = 1e-4\n",
    "theta = 1\n",
    "w, itr_losses, train_accuracies = dp_gradient_descent(iterations, X_train, y_train, epsilon, delta, theta)\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "ax1.plot(np.arange(iterations), itr_losses, label='training loss' )\n",
    "ax2.plot(np.arange(iterations), train_accuracies, label='training accuracy')\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "* (15 points) Given fixed $\\epsilon$ and $\\delta$, the tunable parameters in the training are clipping threshold $\\theta$ and the number of iterations iteration.\n",
    "    - Plot the TESTING accuracies (use the testing dataset) of different thresholds listed below, and discuss the potential trade-offs based on your observation.\n",
    "    - Plot the TESTING accuracies (use the testing dataset) of different thresholds listed below, and discuss the potential trade-offs based on your observation."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "thetas = [0.1, 0.5, 1.0, 1.5, 2.0, 2.5, 3.0]\n",
    "possible_T = [1, 2, 5, 10, 15, 20]\n",
    "default_iterations = 10\n",
    "default_theta = 1\n",
    "epsilon = 2\n",
    "delta = 1e-4\n",
    "\n",
    "# YOUR CODE HERE\n",
    "# train the LR with different parameters\n",
    "# fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "# ax1.plot(thetas, thetas2accuracies, label='training loss' )\n",
    "# ax2.plot(possible_T, T2accuracies, label='training accuracy')\n",
    "# plt.legend()\n",
    "# plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Question 3: Local differential privacy (LDP) (40 points)\n",
    "\n",
    "We can simulate the local differential privacy mechanisms with the adult dataset.\n",
    "The LDP protocols provides strictly stronger privacy protection for user data,\n",
    "but usually suffers decline in terms of utility.\n",
    "\n",
    "* (10 points)\n",
    "The most basic LDP mechanism maybe the Generalized Random Response (GRR),\n",
    "which can help data aggregator to estimate the frequencies of categorical values.\n",
    "Assume that the input domain is $\\mathcal{D}$ and its size $|\\mathcal{D}| = d$.\n",
    "\n",
    "    - (Local randomization) For each user's input, $v$, GRR randomize the input value in the following way\n",
    "\n",
    "    $\n",
    "    Pr[GRR(v) = v'] = \\begin{cases}\n",
    "        p = \\frac{e^{\\epsilon}}{e^{\\epsilon} + d - 1} & \\text{if } v = v' \\\\\n",
    "        q = \\frac{1}{e^{\\epsilon} + d - 1} & \\text{otherwise, i.e. } v'\\in \\mathcal{D}, v\\neq v'\n",
    "    \\end{cases}\n",
    "    $\n",
    "\n",
    "    - (Aggregation & estimate)\n",
    "    After aggregation, one can learn the count of some reported value $v$ and denoted as $C(v)$.\n",
    "    Then the unbiased frequency estimate of $v$ is $\\hat{f}(v) = \\frac{C(v) - n q}{p - q}$.\n",
    "\n",
    "    - Implement the simulation GRR for the \"Education\" attributes.\n",
    "    By simulation, it means you do NOT need to implement the real network code sending/aggregating.\n",
    "    Using only the Numpy function to simulate the randomization and computation is enough, no need to make it complicated."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# preprocess the Education attribute\n",
    "Education_records = adult['Education'].astype(\"category\")\n",
    "print(Education_records)\n",
    "encode_mapping = dict( enumerate(Education_records.cat.categories ) )\n",
    "print(\"encoding mapping:\", encode_mapping)\n",
    "encoded_edu_records = Education_records.cat.codes.to_numpy()\n",
    "print(\"encoded records:\", encoded_edu_records)\n",
    "print(\"true counts:\", np.unique(encoded_edu_records, return_counts=True))\n",
    "\n",
    "def grr_simulation(records, epsilon):\n",
    "    d = len(encode_mapping)\n",
    "    n = len(records)\n",
    "    estimate_freq = np.zeros(d)\n",
    "    # YOUR CODE HERE (randomization part)\n",
    "\n",
    "    # YOUR CODE HERE (unbiased estimation part)\n",
    "\n",
    "    return estimate_freq\n",
    "\n",
    "\n",
    "print(grr_simulation(encoded_edu_records, 2))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "* (10 points)\n",
    "GRR is proved far from the optimal solution when $\\epsilon$ is small or domain size $d$ is large.\n",
    "An improved LDP mechanism call Optimized Unary Encoding (OUE) has been shown outperforms GRR.\n",
    "W.L.G., we assume that the input domain is $\\mathcal{D}=\\{0, 1, 2, \\ldots, d-1\\}$.\n",
    "    - (Local randomization) Unary encoding codes the input value $v$ to a unit vector $\\mathbf{B}$ with only $\\mathbf{B}[v]=1$ and all other elements are set to 0.\n",
    "    OUE randomizes the unary encoding and outputs vector $\\mathbf{B}'$ in the following way:\n",
    "\n",
    "    $\n",
    "    Pr[\\mathbf{B}'[v] = 1] = \\begin{cases}\n",
    "        p = \\frac{1}{2} & \\text{if } \\mathbf{B}[v] = 1 \\\\\n",
    "        q = \\frac{1}{e^{\\epsilon} + 1} & \\text{if } \\mathbf{B}[v] = 0\n",
    "    \\end{cases}\n",
    "    $\n",
    "\n",
    "    - (Aggregation & estimate) The aggregate then can sum all the report vectors. The unbiased frequencies vector is estimated as\n",
    "\n",
    "    $\n",
    "        f = \\frac{\\sum_{i\\in [n]} \\mathbf{B}'_{i} - nq}{p - q}\n",
    "    $\n",
    "\n",
    "    - Implement the simulation of OUE method in the following code block."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def oue_simulation(records, epsilon):\n",
    "    d = len(encode_mapping)\n",
    "    n = len(records)\n",
    "    estimate_freq = np.zeros(d)\n",
    "\n",
    "    # YOUR CODE HERE (randomization part)\n",
    "\n",
    "    # YOUR CODE HERE (unbiased estimation part)\n",
    "    return None\n",
    "\n",
    "# verify whether the results is unbiased\n",
    "results = []\n",
    "for _ in range(50):\n",
    "    results.append(oue_simulation(encoded_edu_records, 1))\n",
    "# print(np.average(results, axis=0))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "* (10 points) Compares the average L2 errors of GRR and OUE with different privacy budgets, plot the results.\n",
    "Summarize your findings."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "epsilons = [0.5, 1.0, 1.5, 2, 2.5, 3.0, 3.5, 4.0]\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "# plt.plot(epsilons, avg_grr_errors, label='GRR' )\n",
    "# plt.plot(epsilons, avg_oue_errors, label='OUE')\n",
    "# plt.legend()\n",
    "# plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "YOUR ANSWER HERE (Summarize your findings.)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "* (10 points) Compares the average L2 errors LDP approaches with the histogram generated by Laplace mechanism.\n",
    "Summarize your findings and try to explain why LDP approaches have poorer utility.\n",
    "    - Hint: you can analyze the variance of the frequency estimates."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# plt.plot(epsilons, avg_grr_errors, label='GRR' )\n",
    "# plt.plot(epsilons, avg_oue_errors, label='OUE')\n",
    "# plt.legend()\n",
    "# plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "YOUR ANSWER HERE (Summarize your findings, explain why)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}